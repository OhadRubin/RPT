rpt_model.RPTConfig:
        vocab_size= 32000
        hidden_size= 1024
        intermediate_size= 2816
        num_hidden_layers= 12
        num_attention_heads= 8
        max_sequence_length= 2048
        initializer_range= 0.02
        rms_norm_eps= 1e-6
        use_cache= True
        tie_word_embeddings= False