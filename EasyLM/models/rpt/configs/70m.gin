rpt_model.RPTConfig:
        vocab_size= 32000
        hidden_size= 512
        intermediate_size= 1280
        num_hidden_layers= 6
        num_attention_heads= 4
        max_sequence_length= 2048
        initializer_range= 0.02
        rms_norm_eps= 1e-6
        use_cache= True
        tie_word_embeddings= False